{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d75147ad",
   "metadata": {},
   "source": [
    "# Universal Bank -WE04 "
   ]
  },
  {
   "cell_type": "raw",
   "id": "1b5e867f",
   "metadata": {},
   "source": [
    "1.0 Import and install python libraries we require"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3137ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "# set random seed to ensure that results are repeatable\n",
    "np.random.seed(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2d0c1c",
   "metadata": {},
   "source": [
    "# 2.0 Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68a33445",
   "metadata": {},
   "outputs": [],
   "source": [
    "ubank=pd.read_csv(\"UniversalBank.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91141db1",
   "metadata": {},
   "source": [
    "# Check the missing values by summing the total na's for each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9c68d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                    0\n",
       "Age                   0\n",
       "Experience            0\n",
       "Income                0\n",
       "ZIP Code              0\n",
       "Family                0\n",
       "CCAvg                 0\n",
       "Education             0\n",
       "Mortgage              0\n",
       "Personal Loan         0\n",
       "Securities Account    0\n",
       "CD Account            0\n",
       "Online                0\n",
       "CreditCard            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ubank.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90733d3b",
   "metadata": {},
   "source": [
    "# Dropping least important columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cd84fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ubank.drop(ubank.columns[[0,4]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4b2931",
   "metadata": {},
   "source": [
    "# Data split -train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d7bb3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into validation and training set\n",
    "train_df, test_df = train_test_split(ubank, test_size=0.3,random_state=1)\n",
    "\n",
    "# to reduce repetition in later code, create variables to represent the columns\n",
    "# that are our predictors and target\n",
    "target = 'CD Account'\n",
    "predictors = list(ubank.columns)\n",
    "predictors.remove(target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99fda425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a standard scaler and fit it to the training set of predictors\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(train_df[predictors])\n",
    "\n",
    "# Transform the predictors of training and test sets\n",
    "train_X = scaler.transform(train_df[predictors]) \n",
    "train_y = train_df[target] \n",
    "\n",
    "test_X = scaler.transform(test_df[predictors])\n",
    "test_y = test_df[target] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58938496",
   "metadata": {},
   "source": [
    "# Logistic Regression using RandomSearch and Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a95069a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "The best recall score is 0.6708245243128964\n",
      "... with parameters: {'solver': 'liblinear', 'penalty': 'l2', 'max_iter': 851, 'C': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "910 fits failed out of a total of 2500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "280 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "340 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1291, in fit\n",
      "    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 63, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\", line 1048, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\", line 864, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\", line 782, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\", line 263, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\", line 263, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 521, in _logistic_regression_path\n",
      "    alpha = (1.0 / C) * (1 - l1_ratio)\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "290 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 71, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.66627907 0.66627907        nan 0.66627907 0.         0.66627907\n",
      " 0.66627907 0.66627907 0.66627907        nan        nan        nan\n",
      "        nan 0.31469345        nan 0.                nan        nan\n",
      " 0.66627907        nan 0.31469345 0.66627907        nan 0.37399577\n",
      " 0.66627907 0.66627907 0.         0.05940803        nan 0.\n",
      "        nan 0.67082452 0.66627907 0.66627907 0.37399577        nan\n",
      " 0.66627907 0.05940803 0.66627907 0.                nan 0.66627907\n",
      "        nan 0.05940803        nan        nan        nan 0.31469345\n",
      "        nan        nan        nan 0.         0.05940803        nan\n",
      "        nan 0.25095137        nan 0.37399577        nan        nan\n",
      " 0.05940803 0.66627907        nan        nan 0.66627907 0.66627907\n",
      " 0.66627907 0.66627907 0.66627907 0.25095137 0.66627907 0.\n",
      "        nan        nan 0.66627907 0.66627907 0.05940803 0.\n",
      "        nan 0.66627907 0.31469345        nan 0.66627907        nan\n",
      "        nan        nan 0.37399577 0.66627907 0.67082452 0.66627907\n",
      " 0.66627907 0.         0.66627907        nan        nan        nan\n",
      "        nan        nan 0.66627907 0.66627907        nan 0.66627907\n",
      "        nan 0.66627907 0.66627907 0.66627907 0.66627907 0.\n",
      " 0.67082452        nan        nan 0.66627907        nan        nan\n",
      "        nan 0.05940803 0.66627907 0.37399577 0.05940803        nan\n",
      "        nan        nan 0.66627907 0.66627907        nan        nan\n",
      " 0.67082452 0.66627907        nan 0.66627907 0.         0.05940803\n",
      " 0.31469345 0.66627907        nan 0.66627907        nan        nan\n",
      " 0.66627907        nan        nan 0.66627907 0.67082452 0.66627907\n",
      " 0.                nan        nan 0.66627907        nan 0.66627907\n",
      " 0.66627907 0.66627907 0.66627907        nan 0.66627907 0.66627907\n",
      " 0.         0.66627907 0.66627907        nan 0.66627907 0.66627907\n",
      "        nan        nan 0.05940803        nan 0.66627907 0.66627907\n",
      " 0.67082452 0.66627907 0.25095137        nan        nan 0.66627907\n",
      " 0.66627907        nan        nan        nan 0.66627907 0.66627907\n",
      "        nan 0.         0.66627907 0.66627907 0.         0.66627907\n",
      "        nan 0.66627907        nan        nan 0.                nan\n",
      "        nan 0.66627907 0.67082452 0.66627907 0.                nan\n",
      " 0.66627907 0.66627907 0.66627907 0.         0.66627907 0.05940803\n",
      " 0.                nan        nan 0.66627907        nan        nan\n",
      " 0.05940803 0.66627907 0.66627907 0.66627907        nan        nan\n",
      " 0.66627907 0.67082452        nan 0.66627907 0.66627907 0.66627907\n",
      " 0.66627907 0.66627907 0.66627907 0.66627907        nan 0.\n",
      "        nan 0.66627907 0.67082452        nan 0.         0.66627907\n",
      " 0.66627907        nan 0.25095137 0.05940803 0.05940803 0.\n",
      "        nan 0.66627907 0.66627907        nan 0.66627907        nan\n",
      " 0.66627907        nan 0.31469345 0.66627907 0.05940803 0.66627907\n",
      " 0.66627907 0.                nan 0.66627907 0.67082452 0.66627907\n",
      "        nan 0.         0.66627907 0.66627907 0.31469345        nan\n",
      " 0.         0.05940803 0.66627907        nan 0.66627907 0.37399577\n",
      " 0.66627907        nan 0.37399577 0.66627907 0.66627907 0.66627907\n",
      " 0.66627907        nan 0.66627907 0.66627907 0.67082452 0.66627907\n",
      " 0.66627907        nan        nan        nan        nan 0.66627907\n",
      " 0.                nan 0.66627907        nan 0.         0.\n",
      " 0.66627907 0.66627907 0.66627907 0.66627907 0.31469345        nan\n",
      " 0.                nan 0.25095137        nan 0.66627907        nan\n",
      " 0.66627907        nan        nan        nan 0.66627907        nan\n",
      " 0.66627907 0.66627907        nan 0.                nan        nan\n",
      " 0.66627907 0.66627907        nan 0.05940803 0.66627907 0.66627907\n",
      " 0.31469345 0.         0.66627907        nan 0.66627907 0.25095137\n",
      " 0.66627907        nan        nan 0.66627907 0.66627907 0.66627907\n",
      "        nan 0.66627907 0.37399577 0.66627907 0.66627907 0.\n",
      " 0.37399577        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.05940803 0.66627907 0.66627907 0.67082452\n",
      " 0.         0.66627907        nan 0.31469345 0.66627907        nan\n",
      "        nan 0.66627907 0.66627907 0.37399577 0.66627907        nan\n",
      "        nan        nan 0.         0.66627907 0.66627907 0.31469345\n",
      " 0.66627907 0.66627907        nan 0.                nan 0.66627907\n",
      " 0.66627907 0.66627907        nan        nan 0.66627907        nan\n",
      " 0.         0.66627907        nan 0.66627907 0.66627907 0.66627907\n",
      " 0.05940803        nan 0.31469345 0.05940803 0.05940803        nan\n",
      " 0.05940803 0.66627907        nan        nan 0.67082452 0.66627907\n",
      " 0.37399577 0.25095137 0.66627907        nan 0.66627907 0.66627907\n",
      " 0.66627907        nan 0.31469345        nan 0.66627907 0.05940803\n",
      " 0.66627907        nan 0.66627907 0.66627907        nan        nan\n",
      " 0.66627907        nan        nan 0.66627907        nan 0.66627907\n",
      " 0.25095137 0.66627907        nan 0.         0.66627907 0.66627907\n",
      " 0.66627907 0.66627907        nan 0.66627907        nan 0.66627907\n",
      " 0.66627907 0.66627907 0.66627907 0.66627907 0.66627907 0.31469345\n",
      " 0.66627907        nan 0.67082452        nan 0.66627907 0.66627907\n",
      " 0.66627907 0.                nan        nan 0.66627907        nan\n",
      " 0.66627907        nan 0.31469345        nan        nan 0.66627907\n",
      " 0.66627907        nan 0.66627907 0.37399577 0.66627907        nan\n",
      " 0.         0.66627907        nan 0.66627907        nan        nan\n",
      " 0.                nan 0.66627907 0.66627907 0.66627907 0.\n",
      "        nan        nan        nan 0.66627907        nan        nan\n",
      "        nan 0.37399577        nan        nan        nan 0.05940803\n",
      "        nan        nan 0.67082452 0.31469345        nan 0.66627907\n",
      " 0.66627907 0.66627907]\n",
      "  warnings.warn(\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the train scores are non-finite: [0.66664286 0.66664286        nan 0.66664286 0.         0.66664286\n",
      " 0.66664286 0.66664286 0.66664286        nan        nan        nan\n",
      "        nan 0.3196039         nan 0.                nan        nan\n",
      " 0.66664286        nan 0.31846104 0.66664286        nan 0.42005195\n",
      " 0.66664286 0.66664286 0.         0.05936364        nan 0.\n",
      "        nan 0.66892857 0.66664286 0.66664286 0.42005195        nan\n",
      " 0.66664286 0.05936364 0.66664286 0.                nan 0.66664286\n",
      "        nan 0.05936364        nan        nan        nan 0.3196039\n",
      "        nan        nan        nan 0.         0.05936364        nan\n",
      "        nan 0.25567532        nan 0.42005195        nan        nan\n",
      " 0.05936364 0.66664286        nan        nan 0.66664286 0.66664286\n",
      " 0.66664286 0.66664286 0.66664286 0.25567532 0.66664286 0.\n",
      "        nan        nan 0.66664286 0.66664286 0.05936364 0.\n",
      "        nan 0.66664286 0.31846104        nan 0.66664286        nan\n",
      "        nan        nan 0.42005195 0.66664286 0.66892857 0.66664286\n",
      " 0.66664286 0.         0.66664286        nan        nan        nan\n",
      "        nan        nan 0.66664286 0.66664286        nan 0.66664286\n",
      "        nan 0.66664286 0.66664286 0.66664286 0.66664286 0.\n",
      " 0.66892857        nan        nan 0.66664286        nan        nan\n",
      "        nan 0.05936364 0.66664286 0.42005195 0.05936364        nan\n",
      "        nan        nan 0.66664286 0.66664286        nan        nan\n",
      " 0.66892857 0.66664286        nan 0.66664286 0.         0.05936364\n",
      " 0.31846104 0.66664286        nan 0.66664286        nan        nan\n",
      " 0.66664286        nan        nan 0.66664286 0.66892857 0.66664286\n",
      " 0.                nan        nan 0.66664286        nan 0.66664286\n",
      " 0.66664286 0.66664286 0.66664286        nan 0.66664286 0.66664286\n",
      " 0.         0.66664286 0.66664286        nan 0.66664286 0.66664286\n",
      "        nan        nan 0.05936364        nan 0.66664286 0.66664286\n",
      " 0.66892857 0.66664286 0.25567532        nan        nan 0.66664286\n",
      " 0.66664286        nan        nan        nan 0.66664286 0.66664286\n",
      "        nan 0.         0.66664286 0.66664286 0.         0.66664286\n",
      "        nan 0.66664286        nan        nan 0.                nan\n",
      "        nan 0.66664286 0.66892857 0.66664286 0.                nan\n",
      " 0.66664286 0.66664286 0.66664286 0.         0.66664286 0.05936364\n",
      " 0.                nan        nan 0.66664286        nan        nan\n",
      " 0.05936364 0.66664286 0.66664286 0.66664286        nan        nan\n",
      " 0.66664286 0.66892857        nan 0.66664286 0.66664286 0.66664286\n",
      " 0.66664286 0.66664286 0.66664286 0.66664286        nan 0.\n",
      "        nan 0.66664286 0.66892857        nan 0.         0.66664286\n",
      " 0.66664286        nan 0.25567532 0.05936364 0.05936364 0.\n",
      "        nan 0.66664286 0.66664286        nan 0.66664286        nan\n",
      " 0.66664286        nan 0.31846104 0.66664286 0.05936364 0.66664286\n",
      " 0.66664286 0.                nan 0.66664286 0.66892857 0.66664286\n",
      "        nan 0.         0.66664286 0.66664286 0.3196039         nan\n",
      " 0.         0.05936364 0.66664286        nan 0.66664286 0.42005195\n",
      " 0.66664286        nan 0.42005195 0.66664286 0.66664286 0.66664286\n",
      " 0.66664286        nan 0.66664286 0.66664286 0.66892857 0.66664286\n",
      " 0.66664286        nan        nan        nan        nan 0.66664286\n",
      " 0.                nan 0.66664286        nan 0.         0.\n",
      " 0.66664286 0.66664286 0.66664286 0.66664286 0.31846104        nan\n",
      " 0.                nan 0.25567532        nan 0.66664286        nan\n",
      " 0.66664286        nan        nan        nan 0.66664286        nan\n",
      " 0.66664286 0.66664286        nan 0.                nan        nan\n",
      " 0.66664286 0.66664286        nan 0.05936364 0.66664286 0.66664286\n",
      " 0.31846104 0.         0.66664286        nan 0.66664286 0.25567532\n",
      " 0.66664286        nan        nan 0.66664286 0.66664286 0.66664286\n",
      "        nan 0.66664286 0.42005195 0.66664286 0.66664286 0.\n",
      " 0.42005195        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.05936364 0.66664286 0.66664286 0.66892857\n",
      " 0.         0.66664286        nan 0.31846104 0.66664286        nan\n",
      "        nan 0.66664286 0.66664286 0.42005195 0.66664286        nan\n",
      "        nan        nan 0.         0.66664286 0.66664286 0.31846104\n",
      " 0.66664286 0.66664286        nan 0.                nan 0.66664286\n",
      " 0.66664286 0.66664286        nan        nan 0.66664286        nan\n",
      " 0.         0.66664286        nan 0.66664286 0.66664286 0.66664286\n",
      " 0.05936364        nan 0.31846104 0.05936364 0.05936364        nan\n",
      " 0.05936364 0.66664286        nan        nan 0.66892857 0.66664286\n",
      " 0.42005195 0.25567532 0.66664286        nan 0.66664286 0.66664286\n",
      " 0.66664286        nan 0.31846104        nan 0.66664286 0.05936364\n",
      " 0.66664286        nan 0.66664286 0.66664286        nan        nan\n",
      " 0.66664286        nan        nan 0.66664286        nan 0.66664286\n",
      " 0.25567532 0.66664286        nan 0.         0.66664286 0.66664286\n",
      " 0.66664286 0.66664286        nan 0.66664286        nan 0.66664286\n",
      " 0.66664286 0.66664286 0.66664286 0.66664286 0.66664286 0.3196039\n",
      " 0.66664286        nan 0.66892857        nan 0.66664286 0.66664286\n",
      " 0.66664286 0.                nan        nan 0.66664286        nan\n",
      " 0.66664286        nan 0.31846104        nan        nan 0.66664286\n",
      " 0.66664286        nan 0.66664286 0.42005195 0.66664286        nan\n",
      " 0.         0.66664286        nan 0.66664286        nan        nan\n",
      " 0.                nan 0.66664286 0.66664286 0.66664286 0.\n",
      "        nan        nan        nan 0.66664286        nan        nan\n",
      "        nan 0.42005195        nan        nan        nan 0.05936364\n",
      "        nan        nan 0.66892857 0.3196039         nan 0.66664286\n",
      " 0.66664286 0.66664286]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {'C':[0.001,0.01,0.1,1,10], # C is the regulization strength\n",
    "               'penalty':['l1', 'l2','elasticnet','none'],\n",
    "              'solver':['saga','liblinear'],\n",
    "              'max_iter': np.arange(500,1000)\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "lr = LogisticRegression()\n",
    "rand_search = RandomizedSearchCV(estimator = lr, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(train_X,train_y)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestlr = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c327c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1600 candidates, totalling 8000 fits\n",
      "The best recall score is 0.6708245243128964\n",
      "... with parameters: {'C': 0.09999999999999998, 'max_iter': 451, 'penalty': 'l2', 'solver': 'liblinear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "4000 fits failed out of a total of 8000.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "4000 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1160, in fit\n",
      "    self._validate_params()\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\sklearn\\base.py\", line 581, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'C' parameter of LogisticRegression must be a float in the range (0, inf]. Got -0.9 instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan ... 0.67082452 0.67082452 0.67082452]\n",
      "  warnings.warn(\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the train scores are non-finite: [       nan        nan        nan ... 0.66892857 0.66892857 0.66892857]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "best_penality = rand_search.best_params_['penalty']\n",
    "best_solver = rand_search.best_params_['solver']\n",
    "min_regulization_strength=rand_search.best_params_['C']\n",
    "min_iter = rand_search.best_params_['max_iter']\n",
    "\n",
    "#Using the best parameters from the Random Search to use as range for the parameters to do the grid search\n",
    "param_grid = {\n",
    "    \n",
    "    'C':np.arange(min_regulization_strength-1,min_regulization_strength+1), \n",
    "               'penalty':[best_penality],\n",
    "              'solver':[best_solver],\n",
    "              'max_iter': np.arange(min_iter-400,min_iter+400)\n",
    "}\n",
    "\n",
    "logreg =  LogisticRegression()\n",
    "grid_search = GridSearchCV(estimator = logreg, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1, # n_jobs=-1 will utilize all available CPUs \n",
    "                return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(train_X,train_y)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestlogreg = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fdad17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.9780000 Precision=1.0000000 Recall=0.6024096 F1=0.7518797\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(test_y, grid_search.predict(test_X))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")\n",
    "Recall_lr={TP/(TP+FN)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af51c31d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic using random &amp; grid search</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.626506</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 model  Accuracy  Precision    Recall  \\\n",
       "0  Logistic using random & grid search     0.974   0.866667  0.626506   \n",
       "\n",
       "         F1  \n",
       "0  0.727273  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance = pd.DataFrame({\"model\": [], \"Accuracy\": [], \"Precision\": [], \"Recall\": [], \"F1\": []})\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"Logistic using random & grid search\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])\n",
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16989158",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb0f54e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 500 candidates, totalling 1500 fits\n",
      "The best recall score is 0.7351598173515982\n",
      "... with parameters: {'kernel': 'poly', 'gamma': 'scale', 'degree': 4, 'coef0': 9, 'C': 20.1}\n"
     ]
    }
   ],
   "source": [
    "#Random search\n",
    "score_measure = \"recall\"\n",
    "kfolds = 3\n",
    "\n",
    "param_grid = {'C':np.arange(0.1,50,10),  #  regularization parameter.\n",
    "               'kernel':['linear', 'rbf','poly'],\n",
    "              'gamma':['scale','auto'],\n",
    "              'degree':np.arange(1,10), #degree is for the polynomial kernal\n",
    "              'coef0':np.arange(1,10) #coef0 is for the polynomial kernal\n",
    "                  \n",
    "}\n",
    "\n",
    "    \n",
    "\n",
    "SVM_R_out = SVC()\n",
    "rand_search = RandomizedSearchCV(estimator = SVM_R_out, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(train_X,train_y)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestSVMrandom = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3a10236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n",
      "The best recall score is 0.7397260273972602\n",
      "... with parameters: {'C': 17.1, 'coef0': 11, 'degree': 4, 'gamma': 'scale', 'kernel': 'poly'}\n"
     ]
    }
   ],
   "source": [
    "#grid search \n",
    "score_measure = \"recall\"\n",
    "kfolds = 3\n",
    "best_kernel = rand_search.best_params_['kernel']\n",
    "best_gamma = rand_search.best_params_['gamma']\n",
    "min_regulization=rand_search.best_params_['C']\n",
    "best_degree = rand_search.best_params_['degree']\n",
    "best_coef0=rand_search.best_params_['coef0']\n",
    "\n",
    "#Using the best parameters from the Random Search to use as range for the parameters to do the grid search\n",
    "param_grid = {\n",
    "    \n",
    "    'C':np.arange(min_regulization-3,min_regulization+3), \n",
    "               'kernel':[best_kernel],\n",
    "              'gamma':[best_gamma],\n",
    "              'degree': np.arange(best_degree-1,best_degree+1),\n",
    "            'coef0': np.arange(best_coef0-3,best_coef0+3)\n",
    "}\n",
    "\n",
    "SVM_G_out = SVC()\n",
    "grid_search = GridSearchCV(estimator = SVM_G_out, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(train_X,train_y)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestSVMgrid = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2625113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.9560000 Precision=0.5934066 Recall=0.6506024 F1=0.6206897\n"
     ]
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(test_y, grid_search.predict(test_X))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")\n",
    "Recall_svm={TP/(TP+FN)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a726f861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logistic using random &amp; grid search</td>\n",
       "      <td>0.978</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.602410</td>\n",
       "      <td>0.75188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>svm using Random &amp; Grid search</td>\n",
       "      <td>0.956</td>\n",
       "      <td>0.593407</td>\n",
       "      <td>0.650602</td>\n",
       "      <td>0.62069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 model  Accuracy  Precision    Recall       F1\n",
       "0  logistic using random & grid search     0.978   1.000000  0.602410  0.75188\n",
       "0       svm using Random & Grid search     0.956   0.593407  0.650602  0.62069"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance= pd.concat([performance, pd.DataFrame({'model':\"svm using Random & Grid search\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])\n",
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56625012",
   "metadata": {},
   "source": [
    "#  Decision Tree classifier using RandomSearch and GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbff7245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "The best recall score is 0.6938689217758987\n",
      "... with parameters: {'min_samples_split': 1, 'min_samples_leaf': 3, 'min_impurity_decrease': 0.0001, 'max_leaf_nodes': 38, 'max_depth': 18, 'criterion': 'gini'}\n"
     ]
    }
   ],
   "source": [
    "#decision tree classifier\n",
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(1,42),  \n",
    "    'min_samples_leaf': np.arange(1,42),\n",
    "    'min_impurity_decrease': np.arange(0.0001, 0.01, 0.0005),\n",
    "    'max_leaf_nodes': np.arange(5, 42), \n",
    "    'max_depth': np.arange(1,42), \n",
    "    'criterion': ['entropy', 'gini'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "rand_search = RandomizedSearchCV(estimator = dtree, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(train_X,train_y)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc93f789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n",
      "The best recall score is 0.6893234672304439\n",
      "... with parameters: {'criterion': 'entropy', 'max_depth': 39, 'max_leaf_nodes': 39, 'min_impurity_decrease': 0.001, 'min_samples_leaf': 3, 'min_samples_split': 5}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"recall\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(3,6),  \n",
    "    'min_samples_leaf': np.arange(3,6),\n",
    "    'min_impurity_decrease': np.arange(0.0009, 0.0012,0.0001),\n",
    "    'max_leaf_nodes': np.arange(36,40), \n",
    "    'max_depth': np.arange(39,42), \n",
    "    'criterion': ['entropy'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(estimator = dtree, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(train_X,train_y)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eec6d3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy=0.9740000 Precision=0.8666667 Recall=0.6265060 F1=0.7272727\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SSVm using random &amp; grid search</td>\n",
       "      <td>0.956</td>\n",
       "      <td>0.593407</td>\n",
       "      <td>0.650602</td>\n",
       "      <td>0.620690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision tree using Random &amp; Grid search</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.626506</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision tree using Random &amp; Grid search</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.626506</td>\n",
       "      <td>0.727273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      model  Accuracy  Precision    Recall  \\\n",
       "0           SSVm using random & grid search     0.956   0.593407  0.650602   \n",
       "0  Decision tree using Random & Grid search     0.974   0.866667  0.626506   \n",
       "0  Decision tree using Random & Grid search     0.974   0.866667  0.626506   \n",
       "\n",
       "         F1  \n",
       "0  0.620690  \n",
       "0  0.727273  \n",
       "0  0.727273  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_matrix = confusion_matrix(test_y, grid_search.predict(test_X))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "print(f\"Accuracy={(TP+TN)/(TP+TN+FP+FN):.7f} Precision={TP/(TP+FP):.7f} Recall={TP/(TP+FN):.7f} F1={2*TP/(2*TP+FP+FN):.7f}\")\n",
    "Recall_lr={TP/(TP+FN)}\n",
    "\n",
    "performance= pd.concat([performance, pd.DataFrame({'model':\"Decision tree using Random & Grid search\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])\n",
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d325dbf",
   "metadata": {},
   "source": [
    "Initially, as a part of data cleansing process, I checked the missing values and found that there are no missing values and since there are no categorical variables, I havent done any encoding. Atlast removed two columns named ID and ZIP code as they are least important and then split the data into train and test.\n",
    "\n",
    "By applying 'recall' as the score measure and using Random & grid search method to find the best fit among the 3 models namely Logistic regression, Support vector machine with 3 kernels and Decision tree.\n",
    "\n",
    "We can conclude that SVM machine with poly kernel is the best fitting model for the universal bank data as it has the highest recall score when compared with Logistic regression and decision tree."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
